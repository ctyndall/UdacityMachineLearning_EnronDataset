{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POI Identifier in Enron Dataset\n",
    "## Chris Tyndall\n",
    "### 10/16/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Enron was a publicly traded American energy company that saw rapid declines in stock price between 2000 and 2001 from \\$90 to less than \\$1 per share, with many employees selling shares in advance of the worst of these declines.  The company filed for bankruptcy shortly after, and several former employees were convicted on fraud and insider trading charges.  Investigations revealed accounting practices that misled the public on the health of the company, and the unexpected bankruptcy was at the time the largest Chapter 11 filing ever.  The subsequent charges, investigations, and finding reveal many employees knew that fraud was occuring.  In addition, the investigations have led to the public release of thousands of emails and financial records on many employees.  These documents make for an interesting dataset to analyze and attempt to classify fraudulent employees.\n",
    "\n",
    "The goal of this project is to analyze this publically available data using machine learning methods to identify persons of interest (POIs) and in the process identify useful features, compare classifiers, and tune them to provide the most useful POI classification.  Enron was not the only company to lose significant value unexpectedly and a miriad of similar events, particularly related to the housing market, led to the Great Recession.  Company fraud and identifying unusual business practice has gained more attention than ever before particularly with the emergence of highly available machine learning techniques.  By understanding the features and patterns that reflect unusual employee activity, appropriate machine learning techniques can be applied to detect fraud within companies before the public damage can occur.\n",
    "\n",
    "---\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "#### Initial dataset\n",
    "\n",
    "The ready-made dataset included 19 features: 14 financial and 5 from the e-mail corpus.  There was also an additional feature for e-mail address that was removed as it has no numerical value.  Of the 14 financial features, 10 were related to monetary compensation (e.g. salary, bonus) and the other 4 were related to stock benefits.  The 5 e-mail features were derived from the e-mail corpus.\n",
    "\n",
    "The dataset is very limited, and there are several missing values. In fact, 62 %  $(\\frac{1708}{2774})$ of the dataset entries (ignoring 'poi' and 'email_address') are \"NaN\".\n",
    "\n",
    "The following people only have 2 features with values:\n",
    "* WODRASKA JOHN\n",
    "* WHALEY DAVID A\n",
    "* WROBEL BRUCE\n",
    "* SCRIMSHAW MATTHEW\n",
    "* GRAMM WENDY L\n",
    "\n",
    "And the following has no values listed:\n",
    "* LOCKHART EUGENE E\n",
    "\n",
    "Eugene is removed from the dataset, as he provides no information.  The other 5 people are not POIs and removing them will reduce the limited dataset even further.  They will be left for now, and removed if their removal drastically affects performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### Outliers\n",
    "\n",
    "The dataset used in this analysis included features extracted from a financial document regarding compensation for employees and the e-mail corpus containing thousands of e-mails from and to employees.  As a result of generating the financial data from the financial document, there were two unnecessary entries:\n",
    "  * TOTAL\n",
    "  * THE TRAVEL AGENCY IN THE PARK\n",
    "  \n",
    "TOTAL was quickly revealed as an outlier when plotting any of the financial features.  It was always the extreme value.  This line item at the bottom of the financial document is the sum of all the payments and should be removed as it does not reflect an invidual.\n",
    "\n",
    "THE TRAVEL AGENCY IN THE PARK did not have an outlier value expect that there was there were only 2 values from both financial and e-mail features which were the 'Other' and 'Total Payments' features of the financial data.  The name suggest this is not an individual, and the footnote on the financial document reveal this was a travel related account.  Because I am trying to identify persons of interest, this non-person has also been removed from the dataset.\n",
    "\n",
    "Another potential outlier was the 'Loan Advance' and 'Total Payments' attributed to Kenneth Lay as it was an unusually large number.  However, only 3 persons in the dataset include 'Loan Advance' and the footnote explains that loan payments were made with stock.  This number is likely accurate considering Kenneth Lay was the CEO, so the outlier was not removed.  The 'Loan Advance' feature itself was not actually used because only 3 persons included this number so the outlier was irrelevant.  This unusually high number did affect the 'Total Payments' value for Lay, and pushed it far above the others at > \\$ 100 million dollars.  Again, this value does reflect the actual (despite absurdly high) value for Lay and was therefore not removed from the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### Feature Removal\n",
    "\n",
    "I took the initial dataset and looked through each feature to get an understanding of the their distribution using a nice Tableau dashboard created by Diego<sup>1</sup>.\n",
    "\n",
    "The two features, **restricted_stock_deferred** and **director_fees**, had no values for any POIs and therefore are not meaningful in distinguishing POIs from non-POIs.  Leaving them could potential cause them to be weighted too heavily since any value other than 'NaN' would indicate a non-POI.\n",
    "\n",
    "The feature **loan_advance** had only 3 total values and was not used.\n",
    " \n",
    "There are also two additional features that I chose to remove from the dataset: **total_payments**, **total_stock_value**.\n",
    "These two features represent the sum of several other features.  Therefore, there is no new information contained within these features, and they are entirely dependent on the other features.  They could arguably replace other individual features since they reflect an underlying total financial compensation.  However, I will remove them from the analysis because princinal component analysis (PCA) should be more effective in extracting this underlying compensation metric while the classifier will appropriately scale the weights for each indiviual component.\n",
    "\n",
    "I also removed the 2 features 'from_poi_to_this_person' and 'from_this_person_to_poi' and replaced them with a ratio to total sent/received e-mails as described below.\n",
    "\n",
    "---\n",
    "\n",
    "#### Feature Addition\n",
    "\n",
    "Many of the distributions within the financial features have large ranges and the appearance of outliers.  However, the data is real, and comparing some features against each other in scatterplots reveals some non-linear relationships (e.g. salary vs bonus).  This means that some features (such as bonus) have non-linear distributions and may correlate to other features in non-linear manners.  I have therefore transformed some of the features using both sqrt and log transformations:\n",
    " * sqrt(bonus)\n",
    " * log(bonus)\n",
    " * sqrt(exercised_stock_options)\n",
    " * log(exercised_stock_options)\n",
    " \n",
    "I also attempted to create some new word features by selecting single common words among POIs that showed up with a higher frequency within e-mails sent from POIs.  I made a word count dictionary from emails that were sent from POIs and created a frequency count by dividing by the total number of e-mail for each POI.  I then looked at the top 200 words from each (ignoring certain stopwords), and found the common words that showed up in the list for multiple POIs.  From this list, I chose words that seemed somewhat meaningful and that a person potentially engaged in fraud may have to use more often.   I selected the words with roots 'enron', 'team', 'want', 'let', 'veri', 'issu', 'provid', and 'depreci' and then extracted the word frequencies for all people in the dataset.  The feature names are added to the dataset using the name of the word root that it counts.\n",
    "\n",
    "It seems that these words are likely common among all employees and simply typically of the business itself. By plotting these features and coloring POI/non-POI, I couldn't see any particular divisions among the word frequencies.  Obviously, the company name 'enron' should appear quite frequently among all e-mails, and it seems that the other words are simply commonly used words across the company.  Before deciding to not use these features, I tested my classifier with and without them included and got better results without them.  This is not too surprising given the common usage of these words among all e-mails. A classifier that attempts to make use of word counts would likely have to use a much larger feature space and include possibly hundreds of different words.\n",
    "\n",
    "Lastly, I added a 2 features that took the ratio of emails from/to POI with respect to the total emails from/to the person.  These features capture the idea that a POI may send and/or receive e-mails from other POIs more frequently better than simply using the total number of messages because some people sent more e-mails.\n",
    "\n",
    "---\n",
    "\n",
    "#### Feature Scaling\n",
    "\n",
    "I used a MinMaxScaler and then PCA to reduce the feature set.  My hope is to extract the correlations introduced with the added sqrt and log financial features from above, and further reduce the general underlying features that likely correlate separately to financial information (both monetary and stock compensation) and e-mail features (general communication among POIs).  By plotting the PCA explained variance parameter, I see that most of the variance is explained with the first 7 components.  There is a little more gain up to 11 components, and very little with the rest (up to all 18 components).  For my parameter tuning, I elected to use PCA and reduce the features to 11 components.\n",
    "\n",
    "---\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Selection\n",
    "\n",
    "#### Initial Comparison\n",
    "I chose 4 algorithms to compare:\n",
    "   * NaiveBayes\n",
    "   * DecisionTree\n",
    "   * KNeighbors\n",
    "   * LogisticRegression\n",
    "   \n",
    "Using the default parameter for these classifiers, there is a range of accuracies from about 60% to 90%.  However, this metric is not too useful because of the unequal distribution of labels in the dataset.  There are only 18 POIs out of 144 people, which means a classifier that simply guesses non-POI will have an accuracy of 87.5%.  Therefore, it is much more useful to consider the precision and recall values when comparing classifiers.  The F1 score provides a nice weighted average of these two scores.\n",
    "\n",
    " Classifier | Precision | Recall | F1 Score\n",
    "--- | ---\n",
    "NaiveBayes |  0.24411\t| 0.59050\t| 0.34542\n",
    "DecisionTree | 0.31235  | 0.31500\t| 0.31367\n",
    "KNeighbors |  0.76991\t| 0.17400\t| 0.28385\t\n",
    "LogisticRegression | 0.26767 | 0.31250 | 0.28835\n",
    "\n",
    "Most of the results are rather poor.  Decision tree does give > 0.3 precision and recall, but I will tune my parameters to achieve better results by tuning the F1 score to the best possible for a range of parameters for DecisionTree, KNeighbors, and LogisticRegression.  NaiveBayes has no parameters to tune and was included in the table as a point of reference.  It actually had the best F1 score when compared to the other classifiers using default parameters.\n",
    "\n",
    "---\n",
    "\n",
    "#### Tuning\n",
    "I performed a GridSearchCV for several parameters on each of the 3 classifiers chosen above.\n",
    "\n",
    "I ran a MinMaxScaler on the features before piping into PCA and reducing the features to 11 components.  The MinMaxScaler ensured that I did not give more weight in variance to the larger values (i.e. the financial features).  As explained above, 11 components seemed reasonable to reduce the features.\n",
    "\n",
    "Due to the limited dataset, and particularly limited POI labels, I used a 10-fold cross-validation for training/testing.  If I simply set aside a test set and trained on a fixed amount of data (say 80%), my results are likely to overfit to the training data and not perform well. Therefore, I made use of the cross-validation methods that GridSearchCV utilizes.\n",
    "\n",
    "Because of the number of 'NaN' in the dataset, I did not use more than 10 fold.  After removal of restricted_stock_deferred, director_fees, and loan_advance, the feature with the smallest number of non-NaN values was deferral_payments with 39.  With 10 folds, this means that the test set would contain likely only 3 or 4 values for this feature and possibly none.  With more folds, the likelyhood of having no data for this feature would only increase, so I left the number of folds at 10.\n",
    "\n",
    "After tuning the parameter for each, I found the following results for each classifer:\n",
    "\n",
    "Classifier | F1 Score\n",
    "--- | ---\n",
    "DecisionTree | 0.185\n",
    "KNeighbors | 0.368\n",
    "LogisticRegression | 0.431\n",
    "\n",
    "The DecisionTree performance actually went down. The PCA possibly hurt the performance of this classifier, or the GridSearchCV was not adequate in properly tuning the classifier due to the limited data.  \n",
    "\n",
    "KNeighbors and LogisticRegression has better performance after tuning with LogisticRegression having the best performance.  Therefore, my final classifier used for evaluation is a logistic regression classifier with the following parameters:\n",
    "\n",
    "LogisticRegression(C=1000000000.0, class_weight='balanced', dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=1e-06, verbose=0, warm_start=True))\n",
    "          \n",
    "---\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "My GridSearchCV optimized parameters based on the 'F1' score.  After finding the best classifier for this score, I ran the tester code supplied with the final project and evaluated accuracy, precision and recall values.\n",
    "\n",
    "Accuracy: 0.77827\n",
    "\n",
    "Precision: 0.33499\t\n",
    "\n",
    "Recall: 0.67300\t\n",
    "\n",
    "F1: 0.44732\n",
    "\n",
    "The accuracy is actually worse than simply stating that all features are non-POIs, which again would yield 87.5% accuracy.  Of course, this would be a useless classifier in an attempt to identify fraudulent persons since it says that no is a POI!  Therefore, accuracy is less important than the precision and recall.  I used F1 to optimize my classifier because it weighs both precision and recall.\n",
    "\n",
    "Precision is defined as: $\\frac{truePositives}{truePositives+falsePositives}$\n",
    "\n",
    "Recall is defined: $\\frac{truePositives}{truePositives+falseNegatives}$\n",
    "\n",
    "F1 is defined : $2*\\frac{precision*recall}{precision + recall}$\n",
    "\n",
    "The precision of my classifier is a bit low, though still above 0.3 while the recall performs better.  Given that I am attempting to identify POIs positively, the recall value may be considered more important than the precision.  A low precision means that there are several false positives and some innocent people are being flagged.  These false positives may be tolerated more than false negatives (missing people who actually were POIs). Considering the definition of POI was 'person of interest' and not as absolutely guilty person, higher importance on recall seems reasonable as it could warrant further potentially exonerating investigation in the case the person was actually innocent.  And who knows, perhaps some guilty Enron people got off the hook without any charges or investigations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[1] https://public.tableau.com/profile/diego2420#!/vizhome/Udacity/UdacityDashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
